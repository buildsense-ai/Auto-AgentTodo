#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Document Processing MCP Server
Provides tools for document template insertion and document list extraction
"""

import os
import json
import logging
import traceback
import hashlib
import tempfile
import re
import subprocess
from datetime import datetime
from typing import Dict, Any, Optional, Union, List
from pathlib import Path

from fastmcp import FastMCP
from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
from openai import OpenAI
import fitz  # PyMuPDF
from docx import Document as DocxDocument

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()  # è‡ªåŠ¨åŠ è½½å½“å‰ç›®å½•ä¸‹çš„.envæ–‡ä»¶
except ImportError:
    pass

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# Initialize FastMCP server
mcp = FastMCP("AI Document Processing Server ğŸ¤–")

# Ensure output directory exists
OUTPUT_DIR = "generated_docs"
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ==================================
# Common Utilities
# ==================================

def get_api_key() -> str:
    """è·å–OpenRouter APIå¯†é’¥"""
    api_key = os.environ.get("OPENROUTER_API_KEY")
    if not api_key:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
        test_mode = os.environ.get("TEST_MODE", "false").lower() == "true"
        if test_mode:
            logger.warning("âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹ŸAPIå¯†é’¥")
            return "test-api-key-for-testing"
        
        logger.error("âŒ æœªæ‰¾åˆ°OPENROUTER_API_KEY")
        raise RuntimeError("ç¼ºå°‘å¿…éœ€çš„APIå¯†é’¥é…ç½®")
    return api_key

class ProcessingError(Exception):
    """è‡ªå®šä¹‰å¤„ç†å¼‚å¸¸"""
    def __init__(self, message: str, error_code: str, status_code: int = 500):
        self.message = message
        self.error_code = error_code
        self.status_code = status_code
        super().__init__(self.message)

# ==================================
# Document Template Insertion
# ==================================

class DocumentExtractor:
    """æ–‡æ¡£å†…å®¹æå–å™¨"""
    
    def __init__(self):
        logger.info("ğŸ“„ æ–‡æ¡£æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_from_file_path(self, file_path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„æå–å†…å®¹"""
        if not os.path.exists(file_path):
            raise ProcessingError(
                f"åŸå§‹æ–‡æ¡£ä¸å­˜åœ¨: {file_path}",
                "FILE_NOT_FOUND",
                404
            )
        return self._extract_content(file_path)
    
    def _extract_content(self, file_path: str) -> str:
        """æå–æ–‡æ¡£å†…å®¹çš„æ ¸å¿ƒæ–¹æ³•"""
        logger.info(f"ğŸ“„ å¼€å§‹æå–æ–‡æ¡£å†…å®¹: {Path(file_path).name}")
        
        content = ""
        
        try:
            file_ext = Path(file_path).suffix.lower()
            
            if file_ext == '.docx':
                doc = DocxDocument(file_path)
                content = "\n".join([para.text for para in doc.paragraphs])
                
                # æå–è¡¨æ ¼å†…å®¹
                for table in doc.tables:
                    for row in table.rows:
                        row_text = " | ".join([cell.text.strip() for cell in row.cells])
                        if row_text.strip():
                            content += f"\nè¡¨æ ¼è¡Œ: {row_text}"
            
            elif file_ext == '.pdf':
                doc = fitz.open(file_path)
                for page in doc:
                    content += page.get_text()
                doc.close()
            
            elif file_ext in ['.txt', '.md']:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            
            else:
                raise ProcessingError(
                    f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}",
                    "UNSUPPORTED_FORMAT",
                    422
                )
            
            if not content.strip():
                raise ProcessingError(
                    "æ–‡æ¡£å†…å®¹ä¸ºç©º",
                    "EMPTY_DOCUMENT",
                    422
                )
            
            logger.info(f"âœ… æˆåŠŸæå–å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            return content.strip()
            
        except ProcessingError:
            raise
        except Exception as e:
            logger.error(f"âŒ æå–æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            raise ProcessingError(
                f"æ–‡æ¡£å†…å®¹æå–å¤±è´¥: {str(e)}",
                "EXTRACTION_ERROR",
                500
            )

class ContentMerger:
    """å†…å®¹æ™ºèƒ½åˆå¹¶å™¨"""
    
    def __init__(self, api_key: str):
        """åˆå§‹åŒ–AIå®¢æˆ·ç«¯"""
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
        )
        self.model = "google/gemini-2.5-pro-preview"
        logger.info("ğŸ§  å†…å®¹åˆå¹¶å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def merge_content(self, template_json: Dict[str, str], original_content: str) -> Dict[str, str]:
        """ä½¿ç”¨AIæ™ºèƒ½åˆå¹¶æ¨¡æ¿JSONå’ŒåŸå§‹å†…å®¹"""
        logger.info("ğŸ§  å¼€å§‹AIæ™ºèƒ½åˆå¹¶...")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
        test_mode = os.environ.get("TEST_MODE", "false").lower() == "true"
        if test_mode or self.client.api_key == "test-api-key-for-testing":
            logger.warning("âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹ŸAIåˆå¹¶")
            return self._mock_merge_content(template_json, original_content)
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£å¤„ç†AIåŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æ¨¡æ¿JSONç»“æ„å’ŒåŸå§‹æ–‡æ¡£å†…å®¹ï¼Œè¿›è¡Œæ™ºèƒ½åˆå¹¶ã€‚

æ¨¡æ¿JSONç»“æ„ï¼š
{json.dumps(template_json, ensure_ascii=False, indent=2)}

åŸå§‹æ–‡æ¡£å†…å®¹ï¼š
{original_content}

ä»»åŠ¡è¦æ±‚ï¼š
1. åˆ†ææ¨¡æ¿JSONä¸­æ¯ä¸ªç« èŠ‚çš„è¦æ±‚
2. ä»åŸå§‹æ–‡æ¡£å†…å®¹ä¸­æå–ç›¸å…³ä¿¡æ¯
3. è¿›è¡Œè¯­ä¹‰åŒ¹é…å’Œå†…å®¹æ•´åˆ
4. ç”Ÿæˆç¬¦åˆæ¨¡æ¿ç»“æ„çš„å†…å®¹

è¾“å‡ºè¦æ±‚ï¼š
- å¿…é¡»è¿”å›JSONæ ¼å¼
- é”®åä¸æ¨¡æ¿JSONå®Œå…¨ä¸€è‡´
- å€¼ä¸ºæ ¹æ®åŸå§‹å†…å®¹æ™ºèƒ½ç”Ÿæˆçš„å…·ä½“å†…å®¹
- å¦‚æœåŸå§‹å†…å®¹ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·åŸºäºåˆç†æ¨æµ‹ç”Ÿæˆå†…å®¹
- æ¯ä¸ªç« èŠ‚å†…å®¹åº”è¯¥å®Œæ•´ã€ä¸“ä¸šã€ç¬¦åˆå®é™…

è¯·ç›´æ¥è¿”å›JSONæ ¼å¼çš„ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ã€‚
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
            )
            
            if not response or not response.choices or not response.choices[0].message.content:
                raise ProcessingError(
                    "AIå“åº”æ— æ•ˆæˆ–ä¸ºç©º",
                    "AI_NO_RESPONSE",
                    500
                )
            
            # æå–JSONå†…å®¹
            response_content = response.choices[0].message.content.strip()
            json_str = self._extract_json_from_response(response_content)
            
            try:
                merged_content = json.loads(json_str)
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
                logger.error(f"AIå“åº”å†…å®¹: {response_content}")
                raise ProcessingError(
                    f"AIè¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {str(e)}",
                    "AI_INVALID_JSON",
                    422
                )
            
            # éªŒè¯åˆå¹¶ç»“æœ
            if not isinstance(merged_content, dict):
                raise ProcessingError(
                    "AIè¿”å›çš„å†…å®¹ä¸æ˜¯å­—å…¸æ ¼å¼",
                    "AI_INVALID_FORMAT",
                    422
                )
            
            logger.info(f"âœ… AIåˆå¹¶æˆåŠŸï¼Œç”Ÿæˆ {len(merged_content)} ä¸ªç« èŠ‚")
            for key, value in merged_content.items():
                preview = str(value)[:100] + "..." if len(str(value)) > 100 else str(value)
                logger.info(f"   ğŸ“ {key}: {preview}")
            
            return merged_content
            
        except ProcessingError:
            raise
        except Exception as e:
            logger.error(f"âŒ AIåˆå¹¶å¤±è´¥: {e}")
            raise ProcessingError(
                f"AIåˆå¹¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "AI_MERGE_ERROR",
                500
            )
    
    def _mock_merge_content(self, template_json: Dict[str, str], original_content: str) -> Dict[str, str]:
        """æ¨¡æ‹ŸAIåˆå¹¶ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰"""
        logger.info("ğŸ§ª æ¨¡æ‹ŸAIåˆå¹¶æ¨¡å¼")
        
        merged_content = {}
        content_lines = original_content.split('\n')
        content_preview = ' '.join(content_lines[:5])[:200]
        
        for key, description in template_json.items():
            # åŸºäºåŸå§‹å†…å®¹å’Œæ¨¡æ¿æè¿°ç”Ÿæˆç®€å•çš„åˆå¹¶å†…å®¹
            merged_content[key] = f"""æ ¹æ®åŸå§‹æ–‡æ¡£å†…å®¹ç”Ÿæˆçš„{key}ç« èŠ‚ï¼š

{description}

åŸºäºåŸå§‹æ–‡æ¡£çš„ç›¸å…³ä¿¡æ¯ï¼š
{content_preview}

æœ¬ç« èŠ‚å†…å®¹å·²æ ¹æ®æ¨¡æ¿è¦æ±‚è¿›è¡Œæ™ºèƒ½æ•´åˆï¼Œç¡®ä¿ç¬¦åˆå·¥ç¨‹æ–‡æ¡£çš„æ ‡å‡†æ ¼å¼å’Œè¦æ±‚ã€‚å…·ä½“å†…å®¹åŒ…æ‹¬é¡¹ç›®çš„åŸºæœ¬æƒ…å†µã€æŠ€æœ¯è¦æ±‚ã€å®æ–½æ–¹æ¡ˆç­‰å…³é”®ä¿¡æ¯ã€‚

æ³¨ï¼šæ­¤å†…å®¹ç”±æµ‹è¯•æ¨¡å¼ç”Ÿæˆï¼Œå®é™…åº”ç”¨ä¸­å°†ä½¿ç”¨çœŸå®AIè¿›è¡Œæ™ºèƒ½åˆå¹¶ã€‚"""
        
        logger.info(f"âœ… æ¨¡æ‹Ÿåˆå¹¶å®Œæˆï¼Œç”Ÿæˆ {len(merged_content)} ä¸ªç« èŠ‚")
        return merged_content
    
    def _extract_json_from_response(self, response_content: str) -> str:
        """ä»AIå“åº”ä¸­æå–JSONå†…å®¹"""
        # å°è¯•æå–JSON
        if "```json" in response_content:
            start = response_content.find("```json") + 7
            end = response_content.find("```", start)
            if end != -1:
                return response_content[start:end].strip()
            else:
                return response_content[response_content.find("```json") + 7:].strip()
        elif response_content.startswith("{") and response_content.endswith("}"):
            return response_content
        else:
            # æŸ¥æ‰¾JSONå¯¹è±¡
            start_idx = response_content.find("{")
            if start_idx != -1:
                brace_count = 0
                for i, char in enumerate(response_content[start_idx:], start_idx):
                    if char == "{":
                        brace_count += 1
                    elif char == "}":
                        brace_count -= 1
                        if brace_count == 0:
                            return response_content[start_idx:i+1]
            return response_content

class DocumentGenerator:
    """æ–‡æ¡£ç”Ÿæˆå™¨"""
    
    def __init__(self):
        logger.info("ğŸ“„ æ–‡æ¡£ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def generate_docx(self, merged_content: Dict[str, str], output_path: str) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆçš„docxæ–‡æ¡£"""
        logger.info("ğŸ“„ å¼€å§‹ç”Ÿæˆdocxæ–‡æ¡£...")
        
        try:
            doc = Document()
            
            # è®¾ç½®æ–‡æ¡£æ ‡é¢˜
            title = doc.add_heading('AIæ™ºèƒ½åˆå¹¶æ–‡æ¡£', 0)
            title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
            
            # æ·»åŠ ç”Ÿæˆæ—¶é—´
            timestamp = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')
            time_para = doc.add_paragraph(f'ç”Ÿæˆæ—¶é—´: {timestamp}')
            time_para.alignment = WD_PARAGRAPH_ALIGNMENT.RIGHT
            
            doc.add_page_break()
            
            # æ·»åŠ ç›®å½•æ ‡é¢˜
            doc.add_heading('ç›®å½•', level=1)
            
            # ç”Ÿæˆç›®å½•
            for i, section_title in enumerate(merged_content.keys(), 1):
                toc_para = doc.add_paragraph(f"{i}. {section_title}")
                toc_para.style = 'List Number'
            
            doc.add_page_break()
            
            # æ·»åŠ æ­£æ–‡å†…å®¹
            for i, (section_title, section_content) in enumerate(merged_content.items(), 1):
                # æ·»åŠ ç« èŠ‚æ ‡é¢˜
                heading = doc.add_heading(f"{i}. {section_title}", level=1)
                
                # æ·»åŠ ç« èŠ‚å†…å®¹
                if isinstance(section_content, str):
                    # å¤„ç†å¤šæ®µè½å†…å®¹
                    paragraphs = section_content.split('\n\n')
                    for para_text in paragraphs:
                        if para_text.strip():
                            para = doc.add_paragraph(para_text.strip())
                            para.style = 'Normal'
                elif isinstance(section_content, list):
                    # å¤„ç†åˆ—è¡¨å†…å®¹
                    for item in section_content:
                        para = doc.add_paragraph(str(item))
                        para.style = 'List Bullet'
                else:
                    # å…¶ä»–ç±»å‹è½¬ä¸ºå­—ç¬¦ä¸²
                    para = doc.add_paragraph(str(section_content))
                    para.style = 'Normal'
                
                # æ·»åŠ ç« èŠ‚é—´è·
                doc.add_paragraph()
            
            # æ·»åŠ é¡µè„š
            footer_section = doc.sections[0]
            footer = footer_section.footer
            footer_para = footer.paragraphs[0]
            footer_para.text = "æœ¬æ–‡æ¡£ç”±AIæ™ºèƒ½åˆå¹¶ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ"
            footer_para.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
            
            # ä¿å­˜æ–‡æ¡£
            doc.save(output_path)
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆdocxæ–‡æ¡£: {output_path}")
            
            # éªŒè¯æ–‡æ¡£å¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯
            validation_info = self._validate_docx(output_path)
            
            return {
                "sections_count": len(merged_content),
                "file_size": os.path.getsize(output_path),
                "validation": validation_info
            }
            
        except ProcessingError:
            raise
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆdocxæ–‡æ¡£å¤±è´¥: {e}")
            raise ProcessingError(
                f"æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {str(e)}",
                "DOCUMENT_GENERATION_ERROR",
                500
            )
    
    def _validate_docx(self, file_path: str) -> Dict[str, Any]:
        """éªŒè¯ç”Ÿæˆçš„docxæ–‡æ¡£"""
        try:
            # å°è¯•æ‰“å¼€æ–‡æ¡£è¿›è¡ŒéªŒè¯
            doc = Document(file_path)
            paragraph_count = len(doc.paragraphs)
            table_count = len(doc.tables)
            
            if paragraph_count == 0:
                raise ProcessingError(
                    "ç”Ÿæˆçš„æ–‡æ¡£ä¸ºç©º",
                    "EMPTY_GENERATED_DOCUMENT",
                    500
                )
            
            validation_info = {
                "paragraph_count": paragraph_count,
                "table_count": table_count,
                "is_valid": True
            }
            
            logger.info(f"âœ… æ–‡æ¡£éªŒè¯é€šè¿‡ï¼ŒåŒ…å« {paragraph_count} ä¸ªæ®µè½")
            return validation_info
            
        except ProcessingError:
            raise
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£éªŒè¯å¤±è´¥: {e}")
            raise ProcessingError(
                f"ç”Ÿæˆçš„æ–‡æ¡£æ ¼å¼æœ‰è¯¯: {str(e)}",
                "DOCUMENT_VALIDATION_ERROR",
                500
            )

# ==================================
# Document List Extraction
# ==================================

class DocumentItem:
    """æ–‡æ¡£é¡¹æ¨¡å‹"""
    def __init__(self, id: str, title: str, level: int = 1, type: str = "heading", parent_id: Optional[str] = None):
        self.id = id
        self.title = title
        self.level = level
        self.type = type
        self.parent_id = parent_id
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "title": self.title,
            "level": self.level,
            "type": self.type,
            "parent_id": self.parent_id
        }

class DocumentListExtractor:
    """æ–‡æ¡£åˆ—è¡¨æå–å™¨"""
    
    def __init__(self):
        self.heading_patterns = [
            r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ã€ï¼.]?\s*(.+)$',
            r'^(\d+(?:\.\d+)*)[ã€ï¼.]?\s*(.+)$',
            r'^[ï¼ˆ(]([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ï¼‰)]\s*(.+)$',
            r'^([A-Za-z]+)[ã€ï¼.]?\s*(.+)$',
            r'^[ï¼ˆ(](\d+)[ï¼‰)]\s*(.+)$',
        ]
        logger.info("ğŸ“‹ æ–‡æ¡£åˆ—è¡¨æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_from_file_path(self, file_path: str) -> List[DocumentItem]:
        """ä»æ–‡ä»¶è·¯å¾„æå–æ–‡æ¡£é¡¹åˆ—è¡¨"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        file_ext = Path(file_path).suffix.lower()
        if file_ext == '.doc':
            docx_path = self._convert_doc_to_docx(file_path)
            return self._extract_from_docx(docx_path)
        elif file_ext == '.docx':
            return self._extract_from_docx(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
    
    def _convert_doc_to_docx(self, doc_path: str) -> str:
        """å°†.docæ–‡ä»¶è½¬æ¢ä¸º.docxæ–‡ä»¶"""
        logger.info("ğŸ”„ å¼€å§‹DOCåˆ°DOCXè½¬æ¢...")
        
        docx_path = doc_path.replace('.doc', '_converted.docx')
        
        try:
            libreoffice_paths = [
                '/Applications/LibreOffice.app/Contents/MacOS/soffice',
                'libreoffice',
                'soffice',
            ]
            
            libreoffice_cmd = None
            for path in libreoffice_paths:
                try:
                    result = subprocess.run([path, '--version'], 
                                          capture_output=True, 
                                          text=True, 
                                          timeout=10)
                    if result.returncode == 0:
                        libreoffice_cmd = path
                        break
                except (FileNotFoundError, subprocess.TimeoutExpired):
                    continue
            
            if not libreoffice_cmd:
                raise RuntimeError("LibreOfficeæœªå®‰è£…æˆ–ä¸å¯ç”¨")
            
            if os.path.exists(docx_path):
                os.remove(docx_path)
            
            cmd = [
                libreoffice_cmd,
                '--headless',
                '--convert-to', 'docx',
                '--outdir', os.path.dirname(doc_path),
                doc_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode != 0:
                raise RuntimeError(f"LibreOfficeè½¬æ¢å¤±è´¥")
            
            expected_docx = doc_path.replace('.doc', '.docx')
            if os.path.exists(expected_docx):
                if expected_docx != docx_path:
                    os.rename(expected_docx, docx_path)
                logger.info(f"âœ… è½¬æ¢æˆåŠŸ: {docx_path}")
                return docx_path
            else:
                raise RuntimeError("è½¬æ¢åçš„æ–‡ä»¶æœªæ‰¾åˆ°")
                
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise RuntimeError(f"æ–‡ä»¶è½¬æ¢å¤±è´¥: {str(e)}")
    
    def _extract_from_docx(self, docx_path: str) -> List[DocumentItem]:
        """ä»docxæ–‡ä»¶æå–æ–‡æ¡£é¡¹åˆ—è¡¨"""
        logger.info(f"ğŸ“„ å¼€å§‹ä»docxæ–‡ä»¶æå–æ–‡æ¡£é¡¹: {Path(docx_path).name}")
        
        try:
            doc = Document(docx_path)
            items = []
            item_counter = 0
            
            # æå–æ®µè½æ ‡é¢˜
            for para in doc.paragraphs:
                if para.text.strip():
                    item = self._process_paragraph(para, item_counter)
                    if item:
                        items.append(item)
                        item_counter += 1
            
            # æå–è¡¨æ ¼æ ‡é¢˜å’Œå†…å®¹
            for table_idx, table in enumerate(doc.tables):
                table_items = self._process_table(table, item_counter, table_idx)
                items.extend(table_items)
                item_counter += len(table_items)
            
            logger.info(f"âœ… æˆåŠŸæå– {len(items)} ä¸ªæ–‡æ¡£é¡¹")
            return items
            
        except Exception as e:
            logger.error(f"âŒ ä»docxæ–‡ä»¶æå–å¤±è´¥: {e}")
            raise RuntimeError(f"æ–‡æ¡£å†…å®¹æå–å¤±è´¥: {str(e)}")
    
    def _process_paragraph(self, para, counter: int) -> Optional[DocumentItem]:
        """å¤„ç†æ®µè½ï¼Œæå–æ ‡é¢˜ä¿¡æ¯"""
        text = para.text.strip()
        
        # è¿‡æ»¤é¡µçœ‰é¡µè„šç­‰æ— å…³å†…å®¹
        if self._is_header_footer(text):
            return None
        
        # å°è¯•åŒ¹é…æ ‡é¢˜æ¨¡å¼
        title_info = self._extract_title_info(text)
        if title_info:
            return DocumentItem(
                id=f"item_{counter}",
                title=title_info['title'],
                level=title_info['level'],
                type="heading"
            )
        
        # å¦‚æœæ˜¯é‡è¦æ®µè½ä½†ä¸æ˜¯æ ‡é¢˜ï¼Œä¹ŸåŒ…å«è¿›æ¥
        if len(text) > 10 and len(text) < 200:
            return DocumentItem(
                id=f"item_{counter}",
                title=text,
                level=3,
                type="paragraph"
            )
        
        return None
    
    def _process_table(self, table, start_counter: int, table_idx: int) -> List[DocumentItem]:
        """å¤„ç†è¡¨æ ¼ï¼Œæå–é‡è¦è¡Œ"""
        items = []
        counter = start_counter
        
        for row_idx, row in enumerate(table.rows):
            row_text = " | ".join([cell.text.strip() for cell in row.cells])
            
            if self._is_important_table_row(row_text):
                items.append(DocumentItem(
                    id=f"table_{table_idx}_row_{row_idx}",
                    title=row_text,
                    level=2,
                    type="table_row",
                    parent_id=f"table_{table_idx}"
                ))
                counter += 1
        
        if items:
            # æ·»åŠ è¡¨æ ¼æ ‡é¢˜
            table_title = DocumentItem(
                id=f"table_{table_idx}",
                title=f"è¡¨æ ¼ {table_idx + 1}",
                level=1,
                type="table"
            )
            return [table_title] + items
        
        return []
    
    def _extract_title_info(self, text: str) -> Optional[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–æ ‡é¢˜ä¿¡æ¯"""
        for pattern in self.heading_patterns:
            match = re.match(pattern, text)
            if match:
                number_part = match.group(1)
                title_part = match.group(2).strip()
                level = self._calculate_level(number_part)
                
                return {
                    'title': f"{number_part}. {title_part}",
                    'level': level,
                    'number': number_part
                }
        
        return None
    
    def _calculate_level(self, number_part: str) -> int:
        """æ ¹æ®ç¼–å·è®¡ç®—å±‚çº§"""
        if re.match(r'^\d+$', number_part):
            return 1
        elif re.match(r'^\d+\.\d+$', number_part):
            return 2
        elif re.match(r'^\d+\.\d+\.\d+$', number_part):
            return 3
        elif number_part in ['ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'ä¸ƒ', 'å…«', 'ä¹', 'å']:
            return 1
        elif re.match(r'^[A-Za-z]$', number_part):
            return 2
        else:
            return 2
    
    def _is_header_footer(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé¡µçœ‰é¡µè„š"""
        header_footer_patterns = [
            r'ç¬¬\s*\d+\s*é¡µ',
            r'å…±\s*\d+\s*é¡µ',
            r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
            r'^é¡µç ',
            r'^ç¬¬.*ç« $',
        ]
        
        for pattern in header_footer_patterns:
            if re.search(pattern, text):
                return True
        
        return len(text) < 5 or len(text) > 300
    
    def _is_important_table_row(self, row_text: str) -> bool:
        """åˆ¤æ–­è¡¨æ ¼è¡Œæ˜¯å¦é‡è¦"""
        if not row_text.strip() or len(row_text) < 5:
            return False
        
        # è¿‡æ»¤æ˜æ˜¾çš„è¡¨å¤´æˆ–åˆ†éš”è¡Œ
        if re.match(r'^[\s\-\|]+$', row_text):
            return False
        
        # åŒ…å«é‡è¦å…³é”®è¯çš„è¡Œ
        important_keywords = ['é¡¹ç›®', 'å†…å®¹', 'è¦æ±‚', 'æ ‡å‡†', 'è§„èŒƒ', 'æ–¹æ¡ˆ', 'æªæ–½']
        for keyword in important_keywords:
            if keyword in row_text:
                return True
        
        return len(row_text) > 10 and len(row_text) < 200

# ==================================
# FastMCP Tools
# ==================================

@mcp.tool()
def insert_template(template_json_input: Union[str, Dict[str, str]], original_file_path: str) -> str:
    """
    AI tool to merge a document with a JSON template to generate a new docx file.
    
    It uses an AI model to intelligently merge content from the original document 
    (e.g., .docx, .pdf, .txt) into the structure defined by the JSON template.

    Args:
        template_json_input: A dictionary or file path for the template JSON.
        original_file_path: The path to the original document file.

    Returns:
        The file path of the generated .docx document.
    """
    logger.info("ğŸš€ Starting template insertion process...")
    
    try:
        # Load template json if a path is provided
        if isinstance(template_json_input, str):
            if not os.path.exists(template_json_input):
                raise FileNotFoundError(f"Template JSON file not found: {template_json_input}")
            with open(template_json_input, 'r', encoding='utf-8') as f:
                template_json = json.load(f)
        else:
            template_json = template_json_input

        # Get API key and initialize components
        api_key = get_api_key()
        extractor = DocumentExtractor()
        merger = ContentMerger(api_key)
        generator = DocumentGenerator()

        # 1. Extract original document content
        original_content = extractor.extract_from_file_path(original_file_path)
        
        # 2. AI intelligent merge
        merged_content = merger.merge_content(template_json, original_content)
        
        # 3. Generate output file path
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_filename = f"merged_document_{timestamp}.docx"
        output_path = os.path.join(OUTPUT_DIR, output_filename)
        
        # 4. Generate docx document
        generation_info = generator.generate_docx(merged_content, output_path)
        
        logger.info(f"âœ… Template insertion process completed successfully. Document saved at: {output_path}")
        return output_path

    except (ProcessingError, FileNotFoundError) as e:
        logger.error(f"âŒ Processing failed: {e}")
        raise
    except Exception as e:
        logger.error(f"âŒ An unexpected error occurred during template insertion: {e}")
        logger.error(traceback.format_exc())
        raise ProcessingError(f"An unexpected error occurred: {str(e)}", "UNEXPECTED_ERROR", 500)

@mcp.tool()
def extract_document_list(file_path: str) -> List[Dict[str, Any]]:
    """
    AI tool to extract a structured list of items from Word documents (.doc/.docx).
    
    This function processes Word documents and extracts headings, paragraphs, and table
    content to create a structured list suitable for dashboard display or further processing.

    Args:
        file_path: Path to the Word document file (.doc or .docx)

    Returns:
        A list of dictionaries containing document items with id, title, level, type, and parent_id
    """
    logger.info(f"ğŸš€ Starting document list extraction from: {file_path}")
    
    try:
        extractor = DocumentListExtractor()
        items = extractor.extract_from_file_path(file_path)
        
        # Convert DocumentItem objects to dictionaries
        result = [item.to_dict() for item in items]
        
        logger.info(f"âœ… Successfully extracted {len(result)} items from document")
        return result
        
    except FileNotFoundError as e:
        logger.error(f"âŒ File not found: {e}")
        raise
    except ValueError as e:
        logger.error(f"âŒ Invalid file format: {e}")
        raise
    except Exception as e:
        logger.error(f"âŒ An unexpected error occurred during extraction: {e}")
        logger.error(traceback.format_exc())
        raise RuntimeError(f"Document extraction failed: {str(e)}")

if __name__ == "__main__":
    print("=" * 70)
    print("ğŸ¤– AI Document Processing MCP Server")
    print("=" * 70)
    print("\nAvailable tools:")
    print("1. insert_template - Merge documents with JSON templates")
    print("2. extract_document_list - Extract structured lists from Word documents")
    print("\nStarting MCP server...")
    print("=" * 70)
    
    mcp.run() 